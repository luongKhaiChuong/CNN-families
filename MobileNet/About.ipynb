{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e5bee1",
   "metadata": {},
   "source": [
    "# 1. Giới thiệu:\n",
    "MobileNet là một họ kiến trúc CNN được phát triển bởi Google, với mục tiêu tối ưu hóa cho các thiết bị di động và hệ thống có tài nguyên hạn chế như điện thoại thông minh, thiết bị IoT hay vi xử lý nhúng. Ý tưởng là tách các kênh trong 1 ảnh ra, học riêng các feature trong từng kênh, sau đó mới gộp lại. Các mô hình trong họ MobileNet giúp giảm thiểu số lượng parameters và lượng phép tính (FLOPs), đồng thời vẫn duy trì độ chính xác cao trong các bài toán thị giác máy tính.\n",
    "\n",
    "Các ứng dụng phổ biến:\n",
    "- Phân loại hình ảnh\n",
    "- Nhận diện đối tượng\n",
    "- Nhận diện khuôn mặt\n",
    "- Phát hiện tư thế\n",
    "- Sử dụng trong các framework như: TensorFlow Lite, CoreML, ONNX,...\n",
    "\n",
    "Dòng này tốt nhất là 128 x 128 -> 224 x 224 (do ImageNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0172f",
   "metadata": {},
   "source": [
    "# 2. Vấn đề:\n",
    "Để dễ mô tả trong các phần dưới, ta quy ước:\n",
    "- M: số kênh đầu vào\n",
    "- N: số kênh đầu ra\n",
    "- K: kích thước kernel (thường là 3)\n",
    "- H × W: kích thước không gian đầu vào\n",
    "\n",
    "Trong CNN truyền thống, một lớp convolution chuẩn sử dụng một tập kernel để trích xuất đặc trưng từ toàn bộ ảnh đầu vào, đồng thời trộn thông tin giữa các kênh màu để tạo ra một đặc trưng tổng hợp.\n",
    "\n",
    "Cách hoạt động: Tạo ra N filter 3D K * K * M. Mỗi filter này trực tiếp duyệt qua hết ảnh ban đầu, vừa học feature xuyên suốt các kênh vừa kết hợp các kênh tạo ra 1 kênh 2D. Việc lặp lại N filter sẽ thu được N kênh.\n",
    "\n",
    "Cách hoạt động của nó gây ra 3 vấn đề chính:\n",
    "\n",
    "### 1. FLOPs:\n",
    "FLOPs cho 1 lớp convolution truyền thống:\n",
    "$$FLOPs=N * [(H*W) * (K*K*M)] = H * W * K^2 * M * N$$\n",
    "\n",
    "Điều này rất tốn kém, đặc biệt khi số kênh lớn. Lấy ví dụ 1 ảnh đầu vào 224x224x3, N = 64, K = 3:\n",
    "\n",
    "$$FLOPs=224*224*3^2*3*64 = 86704128$$\n",
    "\n",
    "### 2. Parameters:\n",
    "Số parameter cũng bị ảnh hưởng nghiêm trọng. Ta có công thức:\n",
    "$$\\text{layer param} = (K*K*M)*N = K^2 * M * N$$\n",
    "\n",
    "Ví dụ với $K=3, M=128, N=256$, ta đã có:\n",
    "$$\\text{layer param} = 3^2*128*256 = 294912$$\n",
    "\n",
    "Đây là số lượng tham số chỉ cho 1 lớp CNN. Vấn đề này khiến bộ nhớ và thời gian tải mô hình tăng mạnh.\n",
    "\n",
    "### 3. Ép 1 bộ lọc làm 2 nhiệm vụ cùng lúc:\n",
    "Khi ta áp dụng cùng 1 kernel cho toàn bộ không gian ảnh, bộ lọc phải làm cùng lúc 2 việc:\n",
    "- Học cách trích xuất các feature không gian (edge, texture,...)\n",
    "- Học cách kết hợp thông tin giữa các kênh màu\n",
    "\n",
    "Việc này khi tách ra sẽ giảm rất nhiều độ phức tạp tính toán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9d834",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Giải pháp đề xuất:\n",
    "### 1. Intuition đằng sau giải pháp: \n",
    "Không phải lớp conv nào cũng cần học quan hệ giữa các kênh cùng 1 lúc. Các feature có thể được trích xuất trong những kênh riêng biệt, rồi mới kết hợp lại như trong xử lý ảnh cổ điển (dùng kernel như Sobel để lọc riêng từng kênh, sau đó mới gộp thông tin)\n",
    "### 2. Depthwise seperable convolution (gọi là DSC cho nhanh):\n",
    "Note: Từ seperable này dùng để chỉ việc tách 2 chức năng riêng biệt trong khối chứ không phải là tách từng kênh để xử lý, vì vốn dĩ depthwise convolution đã tách ra riêng từng filter cho từng kênh rồi.\n",
    "Về cơ bản, conv truyền thống gộp hai nhiệm vụ lọc không gian và kết hợp kênh vào một bước duy nhất. Trong khi đó, DSC phân tách hai nhiệm vụ này ra và sử dụng 2 phép conv để thực hiện thay vì 1.\n",
    "Thay vì dùng convolution 3D thông thường, ta thay bằng 2 bước riêng biệt:\n",
    "- Depthwise convolution (lọc không gian - spatial filtering):\n",
    "    - M filter 2D (thường là $3*3$) được áp dụng tương ứng với M kênh đầu vào, mỗi filter sẽ đảm nhận xử lý 1 kênh.\n",
    "    - Output sẽ là M kênh, giờ đây có chứa những thông tin về spatial feature.\n",
    "    - Intuition kiểu như bạn cho N đứa trẻ tô đậm những phần khác nhau của 1 hình ảnh, thì đầu ra là M ảnh chứa những chi tiết tô đậm riêng biệt.\n",
    "    \n",
    "    VD: ảnh inp có 3 kênh màu -> tensor vẫn có 3 kênh nhờ áp dụng kernel $3*3$ cho mỗi kênh input.\n",
    "- Pointwise convolution (kết hợp kênh - channel aggregation):\n",
    "    - Đây là giai đoạn quyết định số kênh output, bằng cách dùng N filter 3D $1*1*M$ để kết hợp các giá trị tại cùng vị trí không gian nhưng trên nhiều kênh khác nhau.\n",
    "    - Đây là điểm khác biệt lớn nhất của khối này so với phép truyền thống, vì hoàn toàn không học thêm bất cứ gì về quan hệ không gian, chỉ gộp kênh thôi (kiểu nén kênh ấy).\n",
    "\n",
    "    VD: 3 kênh đầu vào (sau deptwise conv) -> 64 kênh output bằng cách dùng 64 kernel $1*1*3$\n",
    "\n",
    "Ta có thể xem ảnh minh họa để hiểu hơn về khối:\n",
    "<img src=\"image0.png\"> \n",
    "Như vậy, số FLOPs sử dụng:\n",
    "$$FLOPs = (H*W)*(K*K*M) + H*W*(N*1*1*M) = H*W*M(K^2+N)$$\n",
    "Lại lấy ví dụ ảnh đầu vào 224x224x3, N = 64, K = 3:\n",
    "$$FLOPs = 224*224*3(3^2+64) = 10988544 \\ll 86704128$$\n",
    "\n",
    "Hiệu quả có thể thấy ngay, giảm gấp $\\approx 7.9$ lần.\n",
    "\n",
    "Việc tính tỉ lệ FLOPs khi áp dụng DSC so với conv được tính theo công thức sau:\n",
    "$$r_{FLOPs} = \\frac{H*W*M(K^2+N)}{H*W*K^2*M*N} = \\frac{K^2+N}{K^2*N} = \\frac{1}{K^2} + \\frac{1}{N}$$\n",
    "\n",
    "Số parameter của DSC:\n",
    "$$\\text{layer param} = (K*K)*M + (1*1*M*N) = M*(K^2+N)$$\n",
    "Lại lấy ví dụ với $K=3, M=128, N=256$, ta đã có:\n",
    "$$\\text{layer param} = 128*(3^2+256) = 33920 \\ll 294912$$\n",
    "\n",
    "Trong ví dụ này, ta có thể thấy số param giảm đến $\\approx 8.7$ lần!\n",
    "\n",
    "Ta cũng tính tỉ lệ số param của DSC so với conv:\n",
    "$$r_{param} = \\frac{M*(K^2+N)}{K^2*M*N} =  \\frac{1}{K^2} + \\frac{1}{N}$$\n",
    "\n",
    "Như vậy, cả FLOPs và số param đều có cùng 1 tỉ lệ giảm. Tỉ lệ này không hề phụ thuộc vào kích thước đầu vào, tạo sự ổn định khi huấn luyện cũng như chạy thực nghiệm.\n",
    "\n",
    "Bây giờ, ta sẽ đi qua các thế hệ của MobileNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7fd5a",
   "metadata": {},
   "source": [
    "# 4. MobileNetv1\n",
    "Là phiên bản đầu tiên sử dụng DSC để giảm mạnh số lượng param cũng như FLOPs so với phép chuẩn.\n",
    "\n",
    "Câu trúc tổng thể:\n",
    "1 conv(3x3, stride=2, out_channel=32) -> BN + ReLU -> 14 Block DSC -> Global average pooling -> FC.\n",
    "\n",
    "Trong cấu trúc, sẽ có 1 số khối DSC có stride = 2 để giảm kích thước.\n",
    "\n",
    "Ngoài ra, Google còn giới thiệu thêm 2 tham số giúp model trở nên linh động hơn nữa:\n",
    "- $\\alpha$: là tham số giảm số kênh trong các khối DSC(max = 1), giúp giảm bớt gánh nặng tính toán. Nên sử dụng trong khoảng [0.5, 1]\n",
    "    \n",
    "    VD: Nếu $\\alpha = 0.75$, thì các out_channel trong các khối DSC sẽ tự động giảm còn 75%.\n",
    "- $\\rho$: là tham số điều chỉnh kích thước đầu vào để tăng tốc độ tính.\n",
    "\n",
    "    VD: Nếu $\\rho = 128$ thì ảnh sẽ được resize xuống còn 128 x 128 trước khi đưa vào architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e6fdf",
   "metadata": {},
   "source": [
    "# 5. MobileNetv2\n",
    "Là bản cải tiến mạnh từ v1, vẫn giữ cốt lõi nhanh và gọn. Được cải tiến từ phát hiện rằng MobileNetv1 khi xuống càng sâu, sự mất mát thông tin ngày càng lớn. Vì thế, lần cải tiến này đã đưa ra 2 cấu trúc mới:\n",
    "- Inverted residual block(IRB): Khối này đã kết hợp 2 yếu tố:\n",
    "    - Inverted: Trong ResNet, khối BottleNeck (dành cho architecture từ 50+ lớp) sẽ được thi triển theo kiểu: Nén -> Xử lý -> Giải nén. Mục đích là để giảm số lượng kênh trước khi vào conv 3x3. Ở v2, người ta cải tiến khối DSC của v1, thêm 1 layer conv 1x1 ở trước nó để tăng kích thước, vì thế thứ tự mới sẽ là: Mở rộng -> Xử lý -> nén lại (bottleneck).\n",
    "\n",
    "    <img src=\"image1.png\">\n",
    "     \n",
    "    - Skip connect: Để xử lý vấn đề hao hụt thông tin do tác dụng phụ DSC, 1 skip connection đã được thêm vào như trong ResNet để bảo toàn thông tin.\n",
    "- Linear bottleneck: Đây chính là phát hiện quan trọng giúp v2 đạt được chính xác cao hơn nữa.\n",
    "    - Khi dữ liệu được cô đọng xuống 1 chiều không gian nhỏ, việc áp dụng các hàm phi tuyến sẽ cắt bỏ đi các giá trị âm, làm giảm đi vùng giá trị có ý nghĩa.\n",
    "    - Vì thế, trong bộ phận nén lại của khối IRB, người ta không áp dụng thêm bất cứ lớp phi tuyến nào cả(linear).\n",
    "\n",
    "Cấu trúc tổng thể: conv(3x3, stride=2)-> IRB x 17 -> 1x1 conv -> Global average pool -> FC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5945266",
   "metadata": {},
   "source": [
    "# 6. MobileNetv3\n",
    "\n",
    "Đây là bản cải tiến lớn từ v2 khi 3 yếu tố mới được giới thiệu:\n",
    "- Squeeze and excitation (SE Block): Nhận thấy không hẳn lớp nào cũng có tầm quan trọng tương đương. Để model dễ tập trung vào thông tin quan trọng hơn, người ta thêm có chế layer-attention như sau:\n",
    "    - Squeeze (Nén):Dùng GAP để nén tất cả thông tin của các kênh, tạo ra 1 vector tóm tắt trọng số của mỗi kênh.\n",
    "    - Excitation (Kích thích/đánh trọng số):Sử dụng 2 lớp FC nhỏ để học mối quan hệ giữa các kênh và tạo ra 1 tập các trọng số (attention score), sau đó áp dụng chúng lại cho từng kênh ban đầu.Điều này giúp model chú ý đến các kênh quan trọng và giảm ảnh hưởng các kênh ít quan trọng hơn.\n",
    "\n",
    "        <img src=\"image2.png\" width=\"900\">\n",
    "\n",
    "- H-swish (hard-swish) activation: Với nhận định rằng những hàm ReLU hay ReLU6 chỉ cắt đi phần âm, giảm đi độ tinh xảo của các tensor, 1 non-linear activation mới được giới thiệu, là bản cải tiến của cả hàm swish lẫn ReLU6:\n",
    "$$\\text{h-swish}(x) = x * ReLU6(x+3)/6$$\n",
    "Hàm phi tuyến này đã được chứng minh là nhẹ hơn swish vốn dùng sigmoid, đồng thời tăng độ chính xác cuối cùng.\n",
    "- Dùng AutoML để tìm kiến trúc tốt nhất: Google đã dùng NAS(Neural architecture search) để lựa chọn:\n",
    "    - số block, số kênh\n",
    "    - kernel_size, stride và loại nonlinear cho từng block\n",
    "    - việc có sử dụng SE cho từng block hay không\n",
    "\n",
    "Sau khi tìm kiếm xong, họ khóa kiến trúc lại thành 2 phiên bản: MobileNetv3-Large và MobileNetv3-Small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7d2ae",
   "metadata": {},
   "source": [
    "# 7. MobileNetv4\n",
    "Là bản mới nhất, mạnh nhất. Được công bố vào thàng 4/2024, đây là 1 bước nhảy vọt không chỉ về hiệu suất, mà còn nhờ cách nó có thể được tối ưu hóa hiệu suất trên nhiều loại phần cứng di động khác nhau. Có 3 sự thay đổi chính:\n",
    "- Universal inverted bottleneck (UIB block): Thay vì chỉ dựa vào 1 loại bottleneck cố định, UIB kết hợp và tổng hợp các ý tưởng tốt nhất từ các architecture trước:\n",
    "    - Inverted bottleneck của v2: Vẫn làm theo phương thức mở rộng -> xử lý bằng Depthwise -> nén lại.\n",
    "    - Các ý tưởng từ ConvNeXt: TLà phiên bản mạnh nhất của CNN, các ý tưởng đã được tiếp thu bao gồm:\n",
    "        - Sử dụng các kernel size lớn hơn cho Depthwise convolution: ConvNeXt đã chứng minh được việc sử dụng các kernel kích thước lớn (7x7, 9x9) ngay cả trong các lớp sâu đã giúp các lớp conv có tầm nhìn rộng hơn trong không gian cục bộ, mô phỏng lại được khả năng nhìn toàn cục self-attention của Transformer.\n",
    "        - Khối FFN (feed-forward network) được tích hợp: Các mô hình Transformer sử dụng 1 khối FFN bao gồm 2 lớp tuyến tính với 1 lớp phi tuyến ở giữa(vd: Linear -> GELU -> Linear) để tăng khả năng biến đổi và biểu diễn của mô hình. V4 đã đưa 1 khối tương tự FFN vào sau khối UIB để mô phỏng khả năng xử lý đặc trưng của Transformer.\n",
    "        - Thay đổi thứ tự chuẩn hóa: v4 đã thử nghiệm với việc thay đổi thứ tự của các lớp và việc sử dụng chuẩn hóa như ConvNeXt.\n",
    "- Mobile MQA (Mobile multi-query attention): v4 đã tối ưu hóa MQA để phù hợp với kiến trúc bộ nhớ và tính toán của các chip di động. Điều này giúp đẩy nhanh tốc độ xử lý các phép attention lên đáng kể(tăng 39% trên Pixel 8 edgeTPU).\n",
    "- Tối ưu hóa quy trình NAS:\n",
    "    - Google đã phát triển 1 quy trình NAS tinh vi hơn, cho phép tìm kiếm kiến trúc hiệu quả và chính xác hơn.\n",
    "    - Khả năng tìm các mô hình là Pareto optimal - là tối ưu về cả độ chính xác và tốc độ trên nhiều nền tảng phần cứng khác nhau.\n",
    "    - Kĩ thuật distillation mới: Google sử dụng 1 kĩ thuật distillation mới để các mô hình v4 học hiệu quả hơn từ các mô hình teacher, kết hợp dữ liệu và các kĩ thuật data augmentation đa dạng giúp đạt được acc cao hơn.\n",
    "\n",
    "Riêng phần này thì cài tay không nổi, vì quá nhiều thành phần mới. Bản implementation official của Google: https://github.com/tensorflow/models/blob/master/official/vision/modeling/backbones/mobilenet.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213b399",
   "metadata": {},
   "source": [
    "# 8. Các biến thể:\n",
    "Ngoài các phiên bản chính, còn có các bản biến thể được điều chỉnh:\n",
    "- Cho thiết bị biên: MobileNetV3EdgeTPU, ...\n",
    "- Đa phương thức: MobileNetMultiMAX, MobileNetMultiMAXSeg,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dc3b9",
   "metadata": {},
   "source": [
    "# 9. Kết luận: \n",
    "Dòng MobileNet mang lại lựa chọn hiệu quả hơn rất nhiều cho các thiết bị tài nguyên thấp, đông thời cũng là cơ sở của nhiều hướng đi và lý thuyết mới."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
