{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ffd2c7",
   "metadata": {},
   "source": [
    "# 1. Giới thiệu:\n",
    "ResNet là 1 cấu trúc được giới thiệu bởi Kaiming He et al. vào năm 2015 trong bài báo: \"Deep Residual Learning for Image Recognition\" [(link)](https://arxiv.org/abs/1512.03385). Ý tưởng chính của paper là học phần dư(residual) thay vì học trực tiếp output của 1 layer, giúp việc huấn luyện các mạng rất sâu (trên 100 lớp) dễ dàng hơn. Dòng này học tốt trên ảnh 3 * 224 * 224, do phần giảm mạnh kích thước ở phần stem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123e577",
   "metadata": {},
   "source": [
    "# 2. Vấn đề:\n",
    "Khi độ sâu của mạng tăng lên, người ta kì vọng rằng mạng sẽ học được các biểu diễn phức tạp hơn, qua đó độ chính xác cũng cao hơn. Tuy nhiên trong thực tế, khi số tầng tăng lên quá nhiều, mạng lại học kém hơn cũng như khó học hơn dù có cùng dữ liệu. Ví dụ như cùng 1 cấu trúc, 1 mạng 20 tầng sẽ có val_acc 80%, trong khi mạng 56 tầng lại chỉ có 75%. Đây là $\\textbf{Degradation problem}$ (suy giảm hiệu suất). Nguyên nhân chính của vấn đề này là do:\n",
    "\n",
    "1. Vanishing gradient problem: Càng vào sâu, gradient backward trong backward propagation sẽ càng ngày càng tiêu biến và mức độ ảnh hưởng nhỏ đi, khiến các lớp đầu của mạng học rất chậm hoặc không học được, khiến mạng không thể tối ưu hiệu quả. \n",
    "Vấn đề này trở nên rõ rệt hơn với các activation layer sigmoid và tanh, vốn trả về các đạo hàm nhỏ hơn 1, khiến gradient gần như tiêu biến khi đến các lớp đầu.\n",
    "\n",
    "2. Optimization difficulty: Mạng quá sâu sẽ dẫn đến việc có quá nhiều tham số để tối ưu, gây hao tổn tài nguyên, cũng như có càng nhiều local minima hoặc saddle point (đạo hàm bằng 0 nhưng không phải cực trị tại khu vực đó, và là thứ nghiêm trọng hơn local minima vì khó tối ưu hơn nhiều) khiến model khó tìm được global minima.\n",
    "\n",
    "3. Khó bỏ qua các lớp không quan trọng: Trong trường hợp có 1 số lớp không giúp cải thiện kết quả, ta nên bỏ qua chúng bằng identity mapping (có nghĩa là output y như input: $H(x) = x$). Nhưng trong các thiết kế truyền thống không có cơ chế này, nên model vẫn sẽ cố học những ánh xạ không cần thiết gây lỗi.\n",
    "\n",
    "Bằng chứng của ResNet paper nằm ở thí nghiệm ở ngay trang 1, khi cả training error và test error của model 56 lớp lại cao hơn cả 1 model chỉ có 20 lớp. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab83ba",
   "metadata": {},
   "source": [
    "# 3. Giải pháp đề xuất:\n",
    "Paper đề xuất 1 lối tư duy mới: Thay vì học hết toàn bộ mọi thứ liên quan đến sự thay đổi của output, hãy chỉ học phần khác biệt giữa input và output sau khi qua các lớp. Ở đây, thay vì học hết ánh xạ $H(x)$, ta lại đi học phần hiệu giữa nó và đầu vào:\n",
    "$$F(x)=H(x)-x$$\n",
    "Hay nói cách khác, $H(x)=F(x)+x$. \n",
    "\n",
    "<img src=\"image0.png\"> \n",
    "\n",
    "Thông tin gốc sẽ được truyền thẳng đến output và skip qua tất cả các layer ở giữa. Đây gọi là skip connection. Cơ chế này giúp ổn định luồng gradient (không dễ suy biến vì luôn đảm bảo gradient tối thiểu thêm 1 $\\frac{\\partial{L}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{y}}(\\frac{\\partial{F(x)}}{\\partial{x}}+1)$), qua đó giảm bớt hiện tượng vanishing gradient nếu như $\\frac{\\partial{F(x)}}{\\partial{x}}$ âm quá lớn và giúp mạng dễ tối ưu hơn qua việc bỏ các layer đi nếu $F(x)$ quá nhỏ. \n",
    "Note: Không nhất thiết là phải luôn +x . Ý tưởng về việc thay đổi hệ số của x cũng như của $F(x)$ đã được thực hiện qua các paper sau: \n",
    "- Weighted residual networks (2016) (Paper: [(link)](https://arxiv.org/pdf/1605.08831)):\n",
    "Cải tiến thay vì $H(x)=F(x)+x$,  thêm 1 phần trọng số vào cuối phép biến đổi x rồi đưa vào activation function.\n",
    "\n",
    "<img src=\"image1.png\">\n",
    "\n",
    "- Fixup Initialization (2019) (Paper: [(link)](https://arxiv.org/pdf/1605.08831)):\n",
    "Cải tiến thay vì $H(x)=F(x)+x$,  thêm 1 phần trọng số vào phần scalar x thành $H(x)=F(x)+\\alpha x$, thêm scale và bias cho các layer và bỏ BatchNorm.\n",
    "- Dynamical isometry for residual networks(2022) (Paper: [https://arxiv.org/pdf/2210.02411]):\n",
    "Biến đổi sao cho tất cả các giá trị riêng của ma trận Jacobian ở đầu vào và ra đều gần 1, giúp ổn định hóa luồng gradient. Bằng cách không random init các parameter mà chọn kiểu init sao cho các thành phần trong block thỏa chuẩn tắc: $\\alpha^2 + \\beta^2 = 1$ cho $H(x)=\\alpha F(x)+\\beta x$ và không cần BatchNorm.\n",
    "\n",
    "Nói hơi dài dòng 1 tí rồi, ta có thể coi skip connection như một kiểu ablation study, khi model tự động đánh giá mức độ quan trọng của các block. Nếu 1 block không đóng góp gì nhiều cho việc giảm lỗi ($F(x)$ quá nhỏ hoặc âm), thì $F(x)$ sẽ được điều chỉnh về 0, đồng nghĩa với việc \"tắt\" đi lớp đó một cách tự động mà không cần con người can thiệp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccec3f",
   "metadata": {},
   "source": [
    "# 4. Các thành phần cơ bản trong ResNet:\n",
    "ResNet được xây dựng từ các khối chính sau:\n",
    "- Residual Block: Là Basic block của ResNet. Cồm có 2 hoặc 3 lớp convolution liên tiếp, mỗi lớp thường gồm:\n",
    "    - Conv layer: thực hiện trích xuất feature\n",
    "    - Batch Normalization (BN): Chuẩn hóa dữ liệu, ổn định phân phối dữ liệu trong quá trình huấn luyện\n",
    "    - Activation function (ReLU): Hàm phi tuyến phổ biến trong ResNet, giúp tăng tính phi tuyến của mạng\n",
    "Kết quả đầu ra của khối sẽ được cộng với input của không qua skip connection:\n",
    "$$y=F(x)+x$$\n",
    "Có 2 kiểu block phổ biến:\n",
    "    - Basic block: 2 conv 3x3, dùng trong ResNet18, ResNet34\n",
    "    - Bottleneck block: conv 1x1 giảm chiều -> conv 3x3 -> conv 1x1 tăng chiều trong ResNet50 trở lên để giảm tham số và tăng hiệu suất tính toán.\n",
    "- Skip connection: Là đường truyền input qua block để cộng trực tiếp với output từ block. Có 2 dạng skip chính:\n",
    "    - Identity shortcut: Khi input và output đã qua block có cùng kích thước, skip connection truyền x giống hệt input.\n",
    "    - Projection shortcut: Khi kích thước output thay đổi, dùng 1 convolution 1x1 để chiếu(project) input về kích thước phù hợp với output trước khi cộng.\n",
    "- Pooling và Fully connected layer (FC):\n",
    "    - Pooling: giảm kích thước ảnh đầu vào, giảm chi phí tính toán.\n",
    "    - FC layer: Ở cuối mạng, chuyển các feature trích xuất được thành classification.\n",
    "\n",
    "Một kiến trúc ResNet điển hình sẽ có :\n",
    "1 conv -> BN -> ReLU -> MaxPool -> n residual block -> Global average pool/Flatten -> FC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d7d2e",
   "metadata": {},
   "source": [
    "# 5. Các phiên bản ResNet cơ bản:\n",
    "Con số trong ResNet chỉ tổng số lớp có trọng số (như conv layer hay FC layer) được sử dụng trong network.\n",
    "- ResNet 18, 34: dùng BasicBlock\n",
    "- ResNet 50, 101, 152: dùng BottleNeck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b91535",
   "metadata": {},
   "source": [
    "# 6. Các biến thể của ResNet:\n",
    "- ResNeXt: nhóm convolution + cardinality \n",
    "- Wide-ResNet: tăng chiều rộng thay vì độ sâu\n",
    "- Res2Net: chia nhỏ tầng conv thành nhiều nhánh nhỏ\n",
    "- ResUNet: dùng trong segmentation, kết hợp U-Net + ResNet\n",
    "- ConvNeXt: giống bản \"ResNet++\", đã có các design pattern của ViT như LayerNorm, GELU, 7x7 depthwise conv, preactivation, không pooling mà dùng down sampling. Nó là bản ResNet mạnh nhất, cũng như là bản CNN mạnh nhất để đấu ViT, dù vẫn thua :)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028e67d",
   "metadata": {},
   "source": [
    "# 7. Kết luận:\n",
    "ResNet là kiến trúc mạng nơ-ron sâu sử dụng cơ chế skip connection giúp giải quyết vấn đề \n",
    "vanishing gradient và suy giảm hiệu suất khi mạng tăng độ sâu. Nhờ đó, ResNet cho phép xây\n",
    "dựng các mô hình rất sâu mà vẫn dễ dàng huấn luyện và đạt hiệu quả cao. Đây là một bước đột \n",
    "phá quan trọng, làm nền tảng cho nhiều kiến trúc mạng hiện đại và ứng dụng trong thị giác máy tính."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
